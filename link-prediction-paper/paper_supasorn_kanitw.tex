\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}
\usepackage{cite}
\usepackage{epsfig, graphics}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Predicting Movies Rating with Parallel Stochastic Gradient Descent without Locks}

\author{
Supasorn Suwajanakorn, Kanit Wongsuphasawat \\
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.} \\
Computer Science \& Engineering\\
University of Washington\\
\texttt{\{supasorn,kanitw\}@cs.washington.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\U}{U}
\newcommand{\M}{M}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

% \begin{abstract}
% Write abstract here
% \end{abstract}

\section{Problem \& Dataset}

Our goal is to predict movie ratings for each user based on previous ratings and movie metadata which includes official genres and user-provided short tags.  Specifically, we plan to implement a learning model based on matrix factorization~\cite{koren:matrix} that addresses the following problems:

\textbf{1) Run-time Performance.}  We aim to explore parallelization techniques that enable fast learning algorithm.  In our initial work, we have developed a simple interference-free parallelization scheme for stochastic gradient descent (SGD) which simultaneously utilizes all available computing resources (cores or nodes) and avoids work overriding.

% We plan to compare our method with Hogwild method by Niu et al.~\cite{niu:hogwild} that uses a shared-memory model without locks to eliminate the locking overhead.

\textbf{2) Cold-Start Problem.}  A common problem for matrix factorization is inability to address the systemâ€™s  newer items (movies in our case) or users.  We aim to address this problem by using a hybrid model combining matrix factorization and content-based filtering techniques using metadata as features.  To address high dimensionality, we plan to compress the dimensionality of tags using hash-kernel techniques~\cite{shi:hashkernels}.

\textbf{3) Class Imbalance.}  It has been shown that in the general context of link prediction and many learning problems, the bias or imbalance of the output class in the training set adversely affects the accuracy of the final predictor. In this work, we will investigate how sensitive rating imbalance affects the outcome of the movie rating prediction. If the accuracy indeed suffers from such imbalance, we plan to follow \cite{menon:link-prediction} and use a relative loss measure to handle imbalance and directly optimizes AUC curve with a modified loss function. The resulting objective can still be optimized with SGD.

We use the MovieLens 20M\footnote{http://grouplens.org/datasets/movielens/}
as our benchmark dataset.  The dataset contains over 20 million ratings and 465 thousand tag applications applied to 27,278 movies by 138,493 users.
All the users included had rated at least 20 movies.  The input rating matrix is sparse since only 0.5\% of the matrix's cells contain values.

In our current progress,  we model the recommendation algorithm using pure matrix
factorization and address the performance problem with an algorithm that parallel stochastic gradient descent without work overriding.




\section{Matrix Factorization with Stochastic Gradient Descent}

We first map the relationship between movies, users and ratings
as a matrix factorization problem.
Each rating is modeled as an inner product between a vector of user and movie in the latent space of rank $k$.  Thus, the estimate of movie $m$'s rating by user $u$ is

\[
  \hat{r}_{um} = \U_u \M_m^T
\]

where $\U_u$ is a rank-$k$ vector that represents row $u$-th of the latent matrix for users $\U$ and $\M_m$ is a rank-$k$ vector that represents row $m$-th of the latent matrix for movies $\M$.

To learn the factor matrices $\U$ and $\M$, we minimize regularized squared error on the set of known ratings.

\begin{align}
  \min_{\U,\M} \sum_{r_{um}} (\U_u \cdot \M_m^T - r_{um})^2
  + \lambda_u \|\U\|^2_F + \lambda_m \|\M\|^2_F
\end{align}

To find solution, we apply stochastic gradient descent using the following update equation

\begin{align}
\left[\begin{array}{c}
\U_u^{(t+1)}
\\
\M_m^{(t+1)}
\end{array}\right]
& \leftarrow
\left[\begin{array}{c}
(1-\eta_t \lambda_u) \U_u^{(t)} - \eta_t \epsilon_t \M_m^{(t)}
\\
(1-\eta_t \lambda_v) \M_m^{(t)} - \eta_t \epsilon_t \U_u^{(t)}
\end{array}\right]
\label{eq:update}
\end{align}

where $\eta_t$ is the step size at time $t$.

\section{Parallel Stochastic Gradient Descent without Interference}

To speed-up matrix factorization, we develop a technique for parallelizing matrix factorization without inference.
In this paper, we refer to each parallel unit as processor, but the same concept is also applicable for distributed system environment.

From (\ref{eq:update}), while the algorithm observe rating $r_{um}$,
it only updates row $\U_u$ and $M_m$.
To minimize interference, we can divide the dataset into subsets such that each movie and each user are updated by one processor at the same time.

One method is to split the input matrix $R$, both vertically and horizontally
into $p$x$p$ submatrices where $p$ is number of processors
(Figure~\ref{fig:split}). We then divide each pass over the entire training
set into $p$ (sub-)iterations. For the first iteration, we distribute $p$ sub
matrices along the diagonal to each processor.  Therefore,   For the next
iterations, for each processor, we assign the submatrix on the right of the
previously assigned submatrix to the process.  If the previous assignment is
the rightmost submatrix, we then assign the leftmost submatrix instead. By
using this assignment method, every movie $m$ and every user $u$ that are
updated by processor p in each iteration will not be updated by other
processor in the same iteration.

\begin{figure}[h]
%\framebox[4.0in]{$\;$}
\centering
\includegraphics[width=4in]{figures/split.pdf}
\caption{\label{fig:split} Paralellizing the matrix with 5 processors.}
\end{figure}

\section{Implementation}

We implement a matrix factorization in python using numpy and scipy and use python's \texttt{multiprocessing} library to spawn multiple processes with shared read-write memory for the latent variable. The number of processes spawned is set to the number of CPU cores.

Since this is a non-convex optimization, we initialize the latent matrices $U$ and $M$ by drawing a sample from a uniform distribution in the range $[0,\sqrt{\frac{\bar{r}}{0.25k}})$ for each entry in the matrices where $\bar{r}$ is the average rating and $k$ is the matrix rank so that $E[\hat{r}_{um}] = \bar{r}$ after initialization.

TODO(supasorn): how do we run this.

\section{Initial Results}

\textbf{Parallel Speed-up.}  Compared to a single-process algorithm, our
parallel version obtains around 20x speed up when run on a 32-core machine. In
principle, the speed should scale up linearly with the number of processes
used. We suspect that this is due to the multiprocess overhead and load
imbalancing and we would hope to see a better speed-up when we run on the
original big dataset.

\textbf{Varying lambda.}  For our preliminary results, we try the simple
matrix factorization problem (no content-based attributes) with different
values of $\lambda$ in $\{0, 0.01, \ldots, 0.2\}$. We run this simultaneously
on a GRAIL cluster which consists of nodes with varying cores from 8 to 32. We
limit the maximum number of iterations to 300 and the time it took varies from
4 to 11 hours depending on the machines and their current loads. A plot of
training and testing RMSE's with respect to $\lambda$ is shown in Figure
\ref{fig:rmselambda}. We have not performed a cross-validation to evaluate the
best set of parameters ($\lambda$, latent rank) -- this will be part of our
final work.


% \section{Next Steps}

% We plan to further find optimal parameters for rank of latent vectors $k$
% (in addition to regularizaiton parameter) using grid search.



% If time allows: other parallezing scheme, temporal dynamics.)

% http://research.yahoo.com/files/kdd-fp074-koren.pdf

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

\bibliographystyle{abbrv}
\bibliography{paper_supasorn_kanitw}{}

\end{document}