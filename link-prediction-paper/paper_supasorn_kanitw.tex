\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
\usepackage{color}
\usepackage{cite}
\usepackage{epsfig, graphics}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Predicting Movies Rating with Parallel Stochastic Gradient Descent without Locks}

\author{
Supasorn Suwajanakorn, Kanit Wongsuphasawat \\
% \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.} \\
Computer Science \& Engineering\\
University of Washington\\
\texttt{\{supasorn,kanitw\}@cs.washington.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\U}{U}
\newcommand{\M}{M}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

% \begin{abstract}
% Write abstract here
% \end{abstract}

\section{Problem \& Dataset}

Our goal is to predict movie ratings for each user based on previous ratings and movie metadata which includes official genres and user-provided short tags.  Specifically, we plan to implement a learning model based on matrix factorization~\cite{koren:matrix} that addresses the following problems:

\textbf{1) Run-time Performance.}  We aim to explore parallelization techniques that enable fast learning algorithm.  In our initial work, we have developed a simple interference-free parallelization scheme for stochastic gradient descent (SGD) which simultaneously utilizes all available computing resources (cores or nodes) and avoids work overriding.

% We plan to compare our method with Hogwild method by Niu et al.~\cite{niu:hogwild} that uses a shared-memory model without locks to eliminate the locking overhead.

\textbf{2) Cold-Start Problem.}  A common problem for matrix factorization is inability to address the systemâ€™s  newer items (movie in our case) or users.  We aim to address this problem by using a hybrid model combining matrix factorization and content-based filtering techniques using metadata as features.  To address high dimensionality

\textbf{3) Class Imbalance.}  It has been shown that in the general context of link prediction and many learning problems, the bias or imbalance of the output class in the training set adversely affects the accuracy of the final predictor. In this work, we will investigate how sensitive rating imbalance affects the outcome of the movie rating prediction. If the accuracy indeed suffers from such imbalance, we plan to follow \cite{menon:link-prediction} and use a relative loss measure to handle imbalance and directly optimizes AUC curve with a modified loss function. The resulting objective can still be optimized with SGD.

We use the MovieLens 20M\footnote{http://grouplens.org/datasets/movielens/}
as our benchmark dataset.  The dataset contains over 20 million ratings and 465 thousand tag applications applied to 27,278 movies by 138,493 users.
All the users included had rated at least 20 movies.  The input rating matrix is sparse since only 0.5\% of the matrix's cells contain values.

In our current progress,  we model the recommendation algorithm using pure matrix
factorization and address the performance problem with an algorithm that parallel stochastic gradient descent without work overriding.




\section{Matrix Factorization with Stochastic Gradient Descent}

We first map the relationship between movies, users and ratings
as a matrix factorization problem.
Each rating is modeled as an inner product between a vector of user and movie in the latent space of rank $k$.  Thus, the estimate of movie $m$'s rating by user $u$ is

\[
  \hat{r}_{um} = \U_u \M_m^T
\]

where $\U_u$ is a rank-$k$ vector that represents row $u$-th of the latent matrix for users $\U$ and $\M_m$ is a rank-$k$ vector that represents row $m$-th of the latent matrix for movies $\M$.

To learn the factor matrices $\U$ and $\M$, we minimize regularized squared error on the set of known ratings.

\begin{align}
  \min_{\U,\M} \sum_{r_{um}} (\U_u \cdot \M_m^T - r_{um})^2
  + \lambda_u \|\U\|^2_F + \lambda_m \|\M\|^2_F
\end{align}

To find solution, we apply stochastic gradient descent using the following update equation

\begin{align}
\left[\begin{array}{c}
\U_u^{(t+1)}
\\
\M_m^{(t+1)}
\end{array}\right]
& \leftarrow
\left[\begin{array}{c}
(1-\eta_t \lambda_u) \U_u^{(t)} - \eta_t \epsilon_t \M_m^{(t)}
\\
(1-\eta_t \lambda_v) \M_m^{(t)} - \eta_t \epsilon_t \U_u^{(t)}
\end{array}\right]
\label{eq:update}
\end{align}

where $\eta_t$ is the step size at time $t$.

\section{Parallel Stochastic Gradient Descent without Interference}

To speed-up matrix factorization, we develop a technique for parallelizing matrix factorization without inference.
In this paper, we refer to each parallel unit as processor, but the same concept is also applicable for distributed system environment.

From (\ref{eq:update}), while the algorithm observe rating $r_{um}$,
it only updates row $\U_u$ and $M_m$.
To minimize interference, we can divide the dataset into subsets such that each movie and each user are updated by one processor at the same time.

One method is to split the input matrix $R$, both vertically and horizontally
into $p$x$p$ submatrices where $p$ is number of processors
(Figure~\ref{fig:split}). We then divide each pass over the entire training
set into $p$ (sub-)iterations. For the first iteration, we distribute $p$ sub
matrices along the diagonal to each processor.  Therefore,   For the next
iterations, for each processor, we assign the submatrix on the right of the
previously assigned submatrix to the process.  If the previous assignment is
the rightmost submatrix, we then assign the leftmost submatrix instead. By
using this assignment method, every movie $m$ and every user $u$ that are
updated by processor p in each iteration will not be updated by other
processor in the same iteration.

\begin{figure}[h]
%\framebox[4.0in]{$\;$}
\centering
\includegraphics[width=4in]{figures/split.pdf}
\caption{\label{fig:split} Paralellizing the matrix with 5 processors.}
\end{figure}

\section{Implementation}

We implement matrix factorization in python using numpy and scipy.
For the parallel version, we use python's \texttt{multiprocessing} library to implement multi-threaded stochastic gradient descent.

We initialize the latent matrices $U$ and $M$ by drawing a sample from a uniform distribution in the range $[0,\sqrt{\frac{\bar{r}}{0.25k}})$ where $\bar{r}$ is the average rating so that $E[\hat{r}_{um}] = \bar{r}$ after initialization.

TODO(supasorn): how do we run this.

\section{Initial Results}

\textbf{Parallel Speed-up.}
TODO(supasorn): How much speed up we've got with parallel (with subset of data-set).

\textbf{Varying lambda.}

TODO(ham):


\section{Next Steps}

We plan to further find optimal parameters for rank of latent vectors $k$
(in addition to regularizaiton parameter) using grid search.

So far we only model the recommendation based on matrix factorization.
We plan to address the cold-start problem by using hybrid-model recommendation techniques by using tags and genres as features of the movies.
As the number of tags are huge, we plan to compress the dimensionality of tags using hash-kernel techniques~\cite{shi:hashkernels}.




% If time allows: other parallezing scheme, temporal dynamics.)

% http://research.yahoo.com/files/kdd-fp074-koren.pdf

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

\bibliographystyle{abbrv}
\bibliography{paper_supasorn_kanitw}{}

\end{document}